{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch-gpu-V\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from database.pymysql_conn import DataBase\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import pickle\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL=\"\"\"\n",
    "SELECT \n",
    "    T1.appid,\n",
    "    T1.avg_player_count,\n",
    "    T1.gameName,\n",
    "    T1.release_date,\n",
    "    T2.publishedAt,\n",
    "    T2.text,    \n",
    "    DATEDIFF(T2.publishedAt, T1.release_date) as datediff\n",
    "FROM\n",
    "    (SELECT \n",
    "        A.appid, A.gameName, A.avg_player_count, B.release_date\n",
    "    FROM\n",
    "        (SELECT \n",
    "        *\n",
    "    FROM\n",
    "        yt.games) A\n",
    "    JOIN (SELECT \n",
    "        appid, name, MAX(release_date) AS release_date\n",
    "    FROM\n",
    "        oasis.app_info2\n",
    "    GROUP BY appid) AS B ON A.appid = B.appid) T1\n",
    "        LEFT JOIN\n",
    "    (SELECT \n",
    "        appid, gameName, text, publishedAt\n",
    "    FROM\n",
    "        steam.yt_comment\n",
    "    WHERE\n",
    "        filter = 0 AND language = 'en') T2 ON T1.appid = T2.appid\n",
    "WHERE\n",
    "    DATEDIFF(T2.publishedAt, T1.release_date) <= 300\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db.to_df(SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en\")\n",
    "\n",
    "def tokenize(text):\n",
    "    # text = remove_stopwords(text)\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1 = df['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "case1=  nltk.word_tokenize(case1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "case2 = tokenize(df['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('agree', 'VBP'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('doaescarlet', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('collect', 'VB'),\n",
       " ('money', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('costumes', 'NNS'),\n",
       " ('by', 'IN'),\n",
       " ('playing', 'VBG'),\n",
       " ('(', '('),\n",
       " ('o', 'JJ'),\n",
       " ('r', 'NN'),\n",
       " (' ', 'NNP'),\n",
       " ('buying', 'VBG'),\n",
       " ('tickets', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('real', 'JJ'),\n",
       " ('money', 'NN'),\n",
       " (')', ')'),\n",
       " ('but', 'CC'),\n",
       " ('at', 'IN'),\n",
       " ('least', 'JJS'),\n",
       " ('you', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('play', 'VB'),\n",
       " ('only', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('spend', 'VB'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('say', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('doa', 'NN'),\n",
       " ('6', 'CD'),\n",
       " ('has', 'VBZ'),\n",
       " ('scandalous', 'JJ'),\n",
       " ('dlc', 'NN'),\n",
       " ('prices', 'NNS'),\n",
       " ('but', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('core', 'NN'),\n",
       " ('game', 'NN'),\n",
       " ('costs', 'VBZ'),\n",
       " ('less', 'JJR'),\n",
       " ('I', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('understand', 'VB'),\n",
       " ('why', 'WRB'),\n",
       " ('tecmo', 'JJ'),\n",
       " ('koei', 'NN'),\n",
       " ('makes', 'VBZ'),\n",
       " ('this', 'DT'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('unfair', 'JJ')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tag.pos_tag(case2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = line\n",
    "# function to test if something is a noun\n",
    "is_noun = lambda pos: 'NN' in pos[:2]\n",
    "is_adj = lambda pos: pos[:2] == 'JJ'\n",
    "# do the nlp stuff\n",
    "tokenized = nltk.word_tokenize(lines)\n",
    "nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "adjectives = [word for (word, pos) in nltk.pos_tag(tokenized) if is_adj(pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "is_adj = lambda pos: pos[:2] == 'JJ'\n",
    "\n",
    "def pos_tagging_noun(text):\n",
    "    ###########\n",
    "    tokenized = tokenize(text)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "    return nouns\n",
    "\n",
    "def pos_tagging_adj(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    adjectives = [word for (word, pos) in nltk.pos_tag(tokenized) if is_adj(pos)]\n",
    "    return adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_count(df):\n",
    "    \n",
    "    nouns = df['text'].progress_apply(pos_tagging_noun)\n",
    "    unpacked = [word for sent in nouns for word in sent]\n",
    "    \n",
    "    noun_cnt = Counter(unpacked)\n",
    "    \n",
    "    return noun_cnt\n",
    "\n",
    "def get_adj_count(df):\n",
    "    \n",
    "    adjs = df['text'].progress_apply(pos_tagging_adj)\n",
    "    unpacked = [word for sent in adjs for word in sent]\n",
    "    \n",
    "    adj_cnt = Counter(unpacked)\n",
    "    \n",
    "    return adj_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 476.27it/s]\n"
     ]
    }
   ],
   "source": [
    "noun_cnt = get_noun_count(df[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_cnt = get_adj_count(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_cnt.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_cnt.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('noun.pickle', 'wb') as f:\n",
    "#     pickle.dump(noun_cnt, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('adj.pickle', 'wb') as f:\n",
    "#     pickle.dump(adj_cnt, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('noun.pickle', 'rb') as f:\n",
    "#     data = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
