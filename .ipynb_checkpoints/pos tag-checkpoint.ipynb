{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch-gpu-V\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from database.pymysql_conn import DataBase\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import pickle\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DataBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL=\"\"\"\n",
    "SELECT \n",
    "    T1.appid,\n",
    "    T1.avg_player_count,\n",
    "    T1.gameName,\n",
    "    T1.release_date,\n",
    "    T2.publishedAt,\n",
    "    T2.text,    \n",
    "    DATEDIFF(T2.publishedAt, T1.release_date) as datediff\n",
    "FROM\n",
    "    (SELECT \n",
    "        A.appid, A.gameName, A.avg_player_count, B.release_date\n",
    "    FROM\n",
    "        (SELECT \n",
    "        *\n",
    "    FROM\n",
    "        yt.games) A\n",
    "    JOIN (SELECT \n",
    "        appid, name, MAX(release_date) AS release_date\n",
    "    FROM\n",
    "        oasis.app_info2\n",
    "    GROUP BY appid) AS B ON A.appid = B.appid) T1\n",
    "        LEFT JOIN\n",
    "    (SELECT \n",
    "        appid, gameName, text, publishedAt\n",
    "    FROM\n",
    "        steam.yt_comment\n",
    "    WHERE\n",
    "        filter = 0 AND language = 'en') T2 ON T1.appid = T2.appid\n",
    "WHERE\n",
    "    DATEDIFF(T2.publishedAt, T1.release_date) <= 300\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db.to_df(SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en\")\n",
    "\n",
    "def tokenize(text):\n",
    "    text = remove_stopwords(text)\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh trust me it got boring soo fast when I first seen it. Ill never buy the 3rd game especially  The DLC. The first and second DOAX is much wayy better because not only it got soo much activities it's what Beach Vacation all about I wish the Nostoligia Bikinis come back and all the other girls in the game. I understand because they want to make a lewd waifu game which it already has been in the last two. I thought the 3rd game going to give us more characters and activities and tournaments for volleyball but got very back to the basic with zoom the titties and lolis.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh trust got boring soo fast I seen it. Ill buy 3rd game especially The DLC. The second DOAX wayy better got soo activities it's Beach Vacation I wish Nostoligia Bikinis come girls game. I understand want lewd waifu game two. I thought 3rd game going characters activities tournaments volleyball got basic zoom titties lolis.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = tokenize(df['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Oh', 'UH'),\n",
       " ('trust', 'NN'),\n",
       " ('got', 'VBD'),\n",
       " ('boring', 'JJ'),\n",
       " ('soo', 'NN'),\n",
       " ('fast', 'NN'),\n",
       " ('I', 'PRP'),\n",
       " ('seen', 'VBN'),\n",
       " ('it', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Ill', 'NNP'),\n",
       " ('buy', 'VB'),\n",
       " ('3rd', 'CD'),\n",
       " ('game', 'NN'),\n",
       " ('especially', 'RB'),\n",
       " ('The', 'DT'),\n",
       " ('DLC', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('second', 'JJ'),\n",
       " ('DOAX', 'NNP'),\n",
       " ('wayy', 'NN'),\n",
       " ('better', 'RBR'),\n",
       " ('got', 'VBD'),\n",
       " ('soo', 'JJ'),\n",
       " ('activities', 'NNS'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('Beach', 'NNP'),\n",
       " ('Vacation', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('Nostoligia', 'NNP'),\n",
       " ('Bikinis', 'NNP'),\n",
       " ('come', 'VBP'),\n",
       " ('girls', 'NNS'),\n",
       " ('game', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('understand', 'VBP'),\n",
       " ('want', 'JJ'),\n",
       " ('lewd', 'NN'),\n",
       " ('waifu', 'JJ'),\n",
       " ('game', 'NN'),\n",
       " ('two', 'CD'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('thought', 'VBD'),\n",
       " ('3rd', 'CD'),\n",
       " ('game', 'NN'),\n",
       " ('going', 'VBG'),\n",
       " ('characters', 'NNS'),\n",
       " ('activities', 'NNS'),\n",
       " ('tournaments', 'NNS'),\n",
       " ('volleyball', 'VBP'),\n",
       " ('got', 'VBD'),\n",
       " ('basic', 'JJ'),\n",
       " ('zoom', 'NN'),\n",
       " ('titties', 'NNS'),\n",
       " ('lolis', 'VBP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tag.pos_tag(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = line\n",
    "# function to test if something is a noun\n",
    "is_noun = lambda pos: 'NN' in pos[:2]\n",
    "is_adj = lambda pos: pos[:2] == 'JJ'\n",
    "# do the nlp stuff\n",
    "tokenized = nltk.word_tokenize(lines)\n",
    "nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "adjectives = [word for (word, pos) in nltk.pos_tag(tokenized) if is_adj(pos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "is_adj = lambda pos: pos[:2] == 'JJ'\n",
    "\n",
    "def pos_tagging_noun(text):\n",
    "    ###########\n",
    "    tokenized = tokenize(text)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "    return nouns\n",
    "\n",
    "def pos_tagging_adj(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    adjectives = [word for (word, pos) in nltk.pos_tag(tokenized) if is_adj(pos)]\n",
    "    return adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_count(df):\n",
    "    \n",
    "    nouns = df['text'].progress_apply(pos_tagging_noun)\n",
    "    unpacked = [word for sent in nouns for word in sent]\n",
    "    \n",
    "    noun_cnt = Counter(unpacked)\n",
    "    \n",
    "    return noun_cnt\n",
    "\n",
    "def get_adj_count(df):\n",
    "    \n",
    "    adjs = df['text'].progress_apply(pos_tagging_adj)\n",
    "    unpacked = [word for sent in adjs for word in sent]\n",
    "    \n",
    "    adj_cnt = Counter(unpacked)\n",
    "    \n",
    "    return adj_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 476.27it/s]\n"
     ]
    }
   ],
   "source": [
    "noun_cnt = get_noun_count(df[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_cnt = get_adj_count(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_cnt.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_cnt.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('noun.pickle', 'wb') as f:\n",
    "#     pickle.dump(noun_cnt, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('adj.pickle', 'wb') as f:\n",
    "#     pickle.dump(adj_cnt, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('noun.pickle', 'rb') as f:\n",
    "#     data = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
